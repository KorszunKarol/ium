{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616c058b",
   "metadata": {},
   "source": [
    "# Etap 2: Feature Engineering for Profitable Location Prediction\n",
    "\n",
    "## Business Problem: Task 12 - \"Where to look for new profitable locations\"\n",
    "\n",
    "**Objective**: Help Nocarz identify the most profitable neighborhoods in London for establishing new Airbnb properties.\n",
    "\n",
    "**Target Variable**: We will predict `annual_revenue_potential` - a refined version of the historical annual revenue that addresses the issues identified in our critical evaluation of Etap 1.\n",
    "\n",
    "**Key Improvements over Etap 1**:\n",
    "1. Better handling of short observation periods\n",
    "2. Seasonality considerations\n",
    "3. More robust price filtering (removing inflated strategic prices)\n",
    "4. Confidence scoring for revenue estimates\n",
    "5. Feature engineering based on 182-day median observation period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cb01a",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "print(\"Setup complete. Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77234eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed datasets\n",
    "processed_data_dir = \"../data/processed/etap2/\"\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "listings_df = pd.read_pickle(processed_data_dir + \"listings_e2_df.pkl\")\n",
    "calendar_df = pd.read_pickle(processed_data_dir + \"calendar_e2_df.pkl\")\n",
    "reviews_df = pd.read_pickle(processed_data_dir + \"reviews_e2_df.pkl\")\n",
    "\n",
    "print(f\"Listings: {listings_df.shape}\")\n",
    "print(f\"Calendar: {calendar_df.shape}\")\n",
    "print(f\"Reviews: {reviews_df.shape}\")\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed72e56",
   "metadata": {},
   "source": [
    "## 2. Robust Revenue Calculation\n",
    "\n",
    "### Addressing Etap 1 Issues:\n",
    "1. **Short observation periods**: Apply minimum observation requirements\n",
    "2. **Inflated strategic pricing**: Filter out extreme prices\n",
    "3. **Naive annualization**: Use observation-period-adjusted calculations\n",
    "4. **Missing confidence measures**: Add reliability scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53058d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_robust_revenue_metrics(calendar_df, min_observation_days=30):\n",
    "    \"\"\"\n",
    "    Calculate robust revenue metrics addressing Etap 1 issues\n",
    "\n",
    "    Parameters:\n",
    "    - min_observation_days: Minimum days of data required for reliable estimates\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with listing-level revenue metrics and confidence scores\n",
    "    \"\"\"\n",
    "    print(f\"Calculating revenue metrics for {len(calendar_df)} calendar records...\")\n",
    "\n",
    "    # Group by listing_id and calculate metrics\n",
    "    listing_metrics = []\n",
    "\n",
    "    for listing_id, group in calendar_df.groupby(\"listing_id\"):\n",
    "        # Basic observation period info\n",
    "        total_days = len(group)\n",
    "        date_range = (group[\"date\"].max() - group[\"date\"].min()).days + 1\n",
    "\n",
    "        # Skip listings with insufficient data\n",
    "        if total_days < min_observation_days:\n",
    "            continue\n",
    "\n",
    "        # Filter out extreme prices (likely strategic inflation)\n",
    "        # Remove prices above 95th percentile or below 5th percentile\n",
    "        price_95 = group[\"price\"].quantile(0.95)\n",
    "        price_05 = group[\"price\"].quantile(0.05)\n",
    "        filtered_group = group[\n",
    "            (group[\"price\"] >= price_05)\n",
    "            & (group[\"price\"] <= price_95)\n",
    "            & (group[\"price\"].notna())\n",
    "        ]\n",
    "\n",
    "        if (\n",
    "            len(filtered_group) < min_observation_days * 0.7\n",
    "        ):  # Need at least 70% valid data\n",
    "            continue\n",
    "\n",
    "        # Calculate occupancy rate (booked days / available days in period)\n",
    "        available_days = len(\n",
    "            filtered_group[filtered_group[\"available\"] == False]\n",
    "        )  # False = booked\n",
    "        total_valid_days = len(filtered_group)\n",
    "        occupancy_rate = (\n",
    "            available_days / total_valid_days if total_valid_days > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Calculate ADR (Average Daily Rate) for booked days only\n",
    "        booked_days = filtered_group[filtered_group[\"available\"] == False]\n",
    "        adr = (\n",
    "            booked_days[\"price\"].mean()\n",
    "            if len(booked_days) > 0\n",
    "            else filtered_group[\"price\"].mean()\n",
    "        )\n",
    "\n",
    "        # Calculate daily revenue (ADR * occupancy)\n",
    "        daily_revenue = adr * occupancy_rate\n",
    "\n",
    "        # Calculate annualized revenue with observation period adjustment\n",
    "        # Instead of naive *365, scale based on actual observation period\n",
    "        if date_range >= 365:\n",
    "            annual_revenue = daily_revenue * 365\n",
    "            confidence_score = 1.0  # Full year of data\n",
    "        else:\n",
    "            # Scale up but with reduced confidence\n",
    "            annual_revenue = daily_revenue * 365\n",
    "            confidence_score = min(\n",
    "                date_range / 365, 0.95\n",
    "            )  # Max 95% confidence for < 1 year\n",
    "\n",
    "        # Additional confidence factors\n",
    "        data_completeness = len(filtered_group) / len(\n",
    "            group\n",
    "        )  # How much data survived filtering\n",
    "        booking_activity = min(\n",
    "            len(booked_days) / 10, 1.0\n",
    "        )  # At least 10 bookings for full confidence\n",
    "\n",
    "        final_confidence = confidence_score * data_completeness * booking_activity\n",
    "\n",
    "        listing_metrics.append(\n",
    "            {\n",
    "                \"listing_id\": listing_id,\n",
    "                \"observation_days\": total_days,\n",
    "                \"date_range_days\": date_range,\n",
    "                \"occupancy_rate\": occupancy_rate,\n",
    "                \"adr\": adr,\n",
    "                \"daily_revenue\": daily_revenue,\n",
    "                \"annual_revenue_potential\": annual_revenue,\n",
    "                \"confidence_score\": final_confidence,\n",
    "                \"booked_days_count\": len(booked_days),\n",
    "                \"data_completeness\": data_completeness,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    result_df = pd.DataFrame(listing_metrics)\n",
    "    print(f\"Calculated metrics for {len(result_df)} listings with sufficient data\")\n",
    "    print(f\"Mean confidence score: {result_df['confidence_score'].mean():.3f}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Calculate revenue metrics\n",
    "revenue_metrics = calculate_robust_revenue_metrics(calendar_df)\n",
    "revenue_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7159f9",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create features that will help predict profitable locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dab70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_property_features(listings_df):\n",
    "    \"\"\"\n",
    "    Engineer property-level features from listings data\n",
    "    \"\"\"\n",
    "    df = listings_df.copy()\n",
    "\n",
    "    # Clean and convert price\n",
    "    if df[\"price\"].dtype == \"object\":\n",
    "        df[\"price_clean\"] = (\n",
    "            df[\"price\"].str.replace(\"[\\$,]\", \"\", regex=True).astype(float)\n",
    "        )\n",
    "    else:\n",
    "        df[\"price_clean\"] = df[\"price\"]\n",
    "\n",
    "    # Property size features\n",
    "    df[\"accommodates\"] = pd.to_numeric(df[\"accommodates\"], errors=\"coerce\")\n",
    "    df[\"bedrooms\"] = pd.to_numeric(df[\"bedrooms\"], errors=\"coerce\")\n",
    "    df[\"beds\"] = pd.to_numeric(df[\"beds\"], errors=\"coerce\")\n",
    "    df[\"bathrooms\"] = pd.to_numeric(\n",
    "        df.get(\"bathrooms_text\", pd.Series()).str.extract(\"(\\d+\\.?\\d*)\", expand=False),\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "\n",
    "    # Create size ratios\n",
    "    df[\"beds_per_bedroom\"] = df[\"beds\"] / df[\"bedrooms\"].replace(0, np.nan)\n",
    "    df[\"accommodates_per_bedroom\"] = df[\"accommodates\"] / df[\"bedrooms\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "\n",
    "    # Property type categories\n",
    "    property_type_mapping = {\n",
    "        \"Entire home/apt\": \"entire_home\",\n",
    "        \"Private room\": \"private_room\",\n",
    "        \"Shared room\": \"shared_room\",\n",
    "        \"Hotel room\": \"hotel_room\",\n",
    "    }\n",
    "    df[\"room_type_encoded\"] = df[\"room_type\"].map(property_type_mapping).fillna(\"other\")\n",
    "\n",
    "    # Review-based features\n",
    "    review_columns = [col for col in df.columns if \"review_scores\" in col]\n",
    "    for col in review_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Overall review quality\n",
    "    if \"review_scores_rating\" in df.columns:\n",
    "        df[\"high_review_score\"] = (df[\"review_scores_rating\"] >= 4.5).astype(int)\n",
    "        df[\"review_score_normalized\"] = df[\"review_scores_rating\"] / 5.0\n",
    "\n",
    "    # Host features\n",
    "    df[\"host_is_superhost\"] = (df.get(\"host_is_superhost\", \"f\") == \"t\").astype(int)\n",
    "    df[\"host_response_rate\"] = (\n",
    "        pd.to_numeric(\n",
    "            df.get(\"host_response_rate\", \"0%\").str.rstrip(\"%\"), errors=\"coerce\"\n",
    "        )\n",
    "        / 100\n",
    "    )\n",
    "\n",
    "    # Instant book feature\n",
    "    df[\"instant_bookable\"] = (df.get(\"instant_bookable\", \"f\") == \"t\").astype(int)\n",
    "\n",
    "    # Location features\n",
    "    df[\"latitude\"] = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "    df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "\n",
    "    # Distance from London center (approximate)\n",
    "    london_center_lat, london_center_lon = 51.5074, -0.1278\n",
    "    df[\"distance_from_center\"] = np.sqrt(\n",
    "        (df[\"latitude\"] - london_center_lat) ** 2\n",
    "        + (df[\"longitude\"] - london_center_lon) ** 2\n",
    "    )\n",
    "\n",
    "    print(f\"Engineered features for {len(df)} listings\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Engineer property features\n",
    "listings_with_features = engineer_property_features(listings_df)\n",
    "print(\"\\nNew features created:\")\n",
    "new_columns = [\n",
    "    col for col in listings_with_features.columns if col not in listings_df.columns\n",
    "]\n",
    "for col in new_columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e257720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_location_features(df, reviews_df):\n",
    "    \"\"\"\n",
    "    Engineer neighborhood-level features\n",
    "    \"\"\"\n",
    "    # Reviews activity by listing\n",
    "    if not reviews_df.empty:\n",
    "        review_stats = (\n",
    "            reviews_df.groupby(\"listing_id\")\n",
    "            .agg({\"date\": [\"count\", \"min\", \"max\"], \"reviewer_id\": \"nunique\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        review_stats.columns = [\n",
    "            \"listing_id\",\n",
    "            \"review_count\",\n",
    "            \"first_review\",\n",
    "            \"last_review\",\n",
    "            \"unique_reviewers\",\n",
    "        ]\n",
    "\n",
    "        # Calculate review velocity (reviews per month)\n",
    "        review_stats[\"review_period_days\"] = (\n",
    "            review_stats[\"last_review\"] - review_stats[\"first_review\"]\n",
    "        ).dt.days\n",
    "        review_stats[\"reviews_per_month\"] = (\n",
    "            review_stats[\"review_count\"] / (review_stats[\"review_period_days\"] / 30.44)\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Merge with main dataframe\n",
    "        df = df.merge(\n",
    "            review_stats[[\"listing_id\", \"review_count\", \"reviews_per_month\"]],\n",
    "            on=\"listing_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        df[\"review_count\"] = df[\"review_count\"].fillna(0)\n",
    "        df[\"reviews_per_month\"] = df[\"reviews_per_month\"].fillna(0)\n",
    "\n",
    "    # Neighborhood-level aggregations\n",
    "    if \"neighbourhood_cleansed\" in df.columns:\n",
    "        neighborhood_stats = (\n",
    "            df.groupby(\"neighbourhood_cleansed\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"price_clean\": [\"mean\", \"median\", \"std\"],\n",
    "                    \"accommodates\": \"mean\",\n",
    "                    \"review_scores_rating\": \"mean\",\n",
    "                    \"id\": \"count\",  # Number of listings in neighborhood\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        neighborhood_stats.columns = [\n",
    "            \"neighbourhood_cleansed\",\n",
    "            \"neighborhood_price_mean\",\n",
    "            \"neighborhood_price_median\",\n",
    "            \"neighborhood_price_std\",\n",
    "            \"neighborhood_accommodates_mean\",\n",
    "            \"neighborhood_review_mean\",\n",
    "            \"neighborhood_listing_count\",\n",
    "        ]\n",
    "\n",
    "        # Calculate neighborhood competition density\n",
    "        neighborhood_stats[\"neighborhood_density\"] = neighborhood_stats[\n",
    "            \"neighborhood_listing_count\"\n",
    "        ]\n",
    "\n",
    "        # Merge back to main dataframe\n",
    "        df = df.merge(neighborhood_stats, on=\"neighbourhood_cleansed\", how=\"left\")\n",
    "\n",
    "        # Relative pricing features\n",
    "        df[\"price_vs_neighborhood\"] = df[\"price_clean\"] / df[\"neighborhood_price_mean\"]\n",
    "        df[\"above_neighborhood_median\"] = (\n",
    "            df[\"price_clean\"] > df[\"neighborhood_price_median\"]\n",
    "        ).astype(int)\n",
    "\n",
    "    print(f\"Location features engineered for {len(df)} listings\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Engineer location features\n",
    "listings_with_features = engineer_location_features(listings_with_features, reviews_df)\n",
    "print(\"\\nLocation features added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488ce25",
   "metadata": {},
   "source": [
    "## 4. Create Final Dataset for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge listings with revenue metrics\n",
    "modeling_df = listings_with_features.merge(\n",
    "    revenue_metrics, left_on=\"id\", right_on=\"listing_id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Final modeling dataset: {modeling_df.shape}\")\n",
    "print(\n",
    "    f\"Target variable (annual_revenue_potential) - Mean: £{modeling_df['annual_revenue_potential'].mean():.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Target variable (annual_revenue_potential) - Median: £{modeling_df['annual_revenue_potential'].median():.2f}\"\n",
    ")\n",
    "\n",
    "# Filter for high-confidence predictions only\n",
    "high_confidence_df = modeling_df[modeling_df[\"confidence_score\"] >= 0.3].copy()\n",
    "print(\n",
    "    f\"\\nHigh-confidence subset: {high_confidence_df.shape} ({len(high_confidence_df) / len(modeling_df) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    # Property characteristics\n",
    "    \"accommodates\",\n",
    "    \"bedrooms\",\n",
    "    \"beds\",\n",
    "    \"bathrooms\",\n",
    "    \"beds_per_bedroom\",\n",
    "    \"accommodates_per_bedroom\",\n",
    "    # Location\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"distance_from_center\",\n",
    "    # Reviews and ratings\n",
    "    \"review_scores_rating\",\n",
    "    \"review_score_normalized\",\n",
    "    \"high_review_score\",\n",
    "    \"review_count\",\n",
    "    \"reviews_per_month\",\n",
    "    # Host features\n",
    "    \"host_is_superhost\",\n",
    "    \"host_response_rate\",\n",
    "    \"instant_bookable\",\n",
    "    # Neighborhood context\n",
    "    \"neighborhood_price_mean\",\n",
    "    \"neighborhood_price_median\",\n",
    "    \"neighborhood_listing_count\",\n",
    "    \"neighborhood_review_mean\",\n",
    "    \"price_vs_neighborhood\",\n",
    "    \"above_neighborhood_median\",\n",
    "    # Revenue calculation metadata\n",
    "    \"occupancy_rate\",\n",
    "    \"adr\",\n",
    "    \"confidence_score\",\n",
    "]\n",
    "\n",
    "# Add categorical features (will be encoded later)\n",
    "categorical_features = [\"room_type_encoded\", \"neighbourhood_cleansed\"]\n",
    "\n",
    "# Create feature matrix\n",
    "available_features = [\n",
    "    col for col in feature_columns if col in high_confidence_df.columns\n",
    "]\n",
    "available_categorical = [\n",
    "    col for col in categorical_features if col in high_confidence_df.columns\n",
    "]\n",
    "\n",
    "print(f\"\\nAvailable numerical features: {len(available_features)}\")\n",
    "print(f\"Available categorical features: {len(available_categorical)}\")\n",
    "\n",
    "# Check for missing values in key features\n",
    "print(\"\\nMissing values in key features:\")\n",
    "for col in available_features + available_categorical:\n",
    "    missing_pct = high_confidence_df[col].isna().mean() * 100\n",
    "    if missing_pct > 0:\n",
    "        print(f\"{col}: {missing_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39240e4b",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modeling dataset\n",
    "output_path = \"../data/processed/etap2/modeling_dataset.pkl\"\n",
    "high_confidence_df.to_pickle(output_path)\n",
    "print(f\"Modeling dataset saved to: {output_path}\")\n",
    "\n",
    "# Save feature configuration\n",
    "feature_config = {\n",
    "    \"numerical_features\": available_features,\n",
    "    \"categorical_features\": available_categorical,\n",
    "    \"target_variable\": \"annual_revenue_potential\",\n",
    "    \"confidence_threshold\": 0.3,\n",
    "    \"total_samples\": len(high_confidence_df),\n",
    "    \"feature_count\": len(available_features) + len(available_categorical),\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "config_path = \"../data/processed/etap2/feature_config.json\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "print(f\"Feature configuration saved to: {config_path}\")\n",
    "\n",
    "print(\"\\n=== Feature Engineering Complete ===\")\n",
    "print(f\"Dataset ready for modeling: {high_confidence_df.shape}\")\n",
    "print(f\"Target variable: {feature_config['target_variable']}\")\n",
    "print(f\"Total features: {feature_config['feature_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1feea0f",
   "metadata": {},
   "source": [
    "## 6. Data Summary for Modeling\n",
    "\n",
    "### Key Improvements over Etap 1:\n",
    "1. **Robust Revenue Calculation**: Addresses short observation periods and inflated pricing\n",
    "2. **Confidence Scoring**: Each revenue estimate has an associated confidence level\n",
    "3. **Feature Engineering**: Property, location, and neighborhood context features\n",
    "4. **Data Quality**: High-confidence subset ensures reliable model training\n",
    "\n",
    "### Next Steps:\n",
    "1. **Baseline Model**: Simple linear regression or decision tree\n",
    "2. **Advanced Model**: Random Forest, Gradient Boosting, or Neural Network\n",
    "3. **Model Comparison**: Performance metrics and business impact analysis\n",
    "4. **Microservice Implementation**: API for real-time predictions\n",
    "5. **A/B Testing Framework**: Compare model predictions vs current methods"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
