{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946dc21f",
   "metadata": {},
   "source": [
    "# Etap 2: Data Load and Process\n",
    "\n",
    "## Overview\n",
    "This notebook implements Phase 1 of the Etap 2 analysis plan: basic data loading, initial processing, and pickle file creation for the new data batch.\n",
    "\n",
    "## Objectives\n",
    "- Load and clean listings.csv, calendar.csv, and reviews.csv\n",
    "- Create standardized pickle files for efficient subsequent analysis\n",
    "- Perform basic validation and data integrity checks\n",
    "- Document data quality improvements compared to Etap 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5f1efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data directory '../data/processed/etap2/' ensured.\n",
      "âœ“ Setup completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths for Etap 2 data\n",
    "listings_path_e2 = '../data/raw2/listings.csv'\n",
    "calendar_path_e2 = '../data/raw2/calendar.csv'\n",
    "reviews_path_e2 = '../data/raw2/reviews.csv'\n",
    "\n",
    "# Define paths for processed (pickled) data\n",
    "processed_data_dir_e2 = '../data/processed/etap2/'\n",
    "listings_pkl_path_e2 = os.path.join(processed_data_dir_e2, 'listings_e2_df.pkl')\n",
    "calendar_pkl_path_e2 = os.path.join(processed_data_dir_e2, 'calendar_e2_df.pkl')\n",
    "reviews_pkl_path_e2 = os.path.join(processed_data_dir_e2, 'reviews_e2_df.pkl')\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "os.makedirs(processed_data_dir_e2, exist_ok=True)\n",
    "print(f\"Processed data directory '{processed_data_dir_e2}' ensured.\")\n",
    "\n",
    "# Define validation schemas\n",
    "EXPECTED_COLUMNS = {\n",
    "    'listings': ['id', 'name', 'host_id', 'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'price'],\n",
    "    'calendar': ['listing_id', 'date', 'available', 'price'],\n",
    "    'reviews': ['listing_id', 'id', 'date', 'reviewer_id', 'reviewer_name', 'comments']\n",
    "}\n",
    "\n",
    "MIN_EXPECTED_ROWS = {\n",
    "    'listings': 10000,\n",
    "    'calendar': 100000,\n",
    "    'reviews': 50000\n",
    "}\n",
    "\n",
    "print(\"âœ“ Setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "140c2bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def check_file_freshness(csv_path, pickle_path):\n",
    "    \"\"\"Check if pickle file is newer than CSV file.\"\"\"\n",
    "    if not os.path.exists(csv_path) or not os.path.exists(pickle_path):\n",
    "        return False\n",
    "    return os.path.getmtime(pickle_path) > os.path.getmtime(csv_path)\n",
    "\n",
    "def validate_dataframe(df, name, expected_cols, min_rows):\n",
    "    \"\"\"Validate dataframe structure and content.\"\"\"\n",
    "    validation = {\n",
    "        'valid': True,\n",
    "        'warnings': [],\n",
    "        'errors': []\n",
    "    }\n",
    "\n",
    "    # Check minimum rows\n",
    "    if len(df) < min_rows:\n",
    "        validation['warnings'].append(f\"Only {len(df)} rows, expected at least {min_rows}\")\n",
    "\n",
    "    # Check for expected columns\n",
    "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        validation['warnings'].append(f\"Missing expected columns: {missing_cols}\")\n",
    "\n",
    "    # Check for completely empty columns\n",
    "    empty_cols = [col for col in df.columns if df[col].isna().all()]\n",
    "    if empty_cols:\n",
    "        validation['warnings'].append(f\"Completely empty columns: {empty_cols}\")\n",
    "\n",
    "    # Check for duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        validation['warnings'].append(f\"{duplicates} duplicate rows found\")\n",
    "\n",
    "    return validation\n",
    "\n",
    "def get_missing_value_summary(df, name):\n",
    "    \"\"\"Get comprehensive missing value summary.\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'count': missing_counts,\n",
    "        'percentage': missing_pct\n",
    "    })\n",
    "\n",
    "    return summary[summary['count'] > 0].sort_values('percentage', ascending=False)\n",
    "\n",
    "print(\"âœ“ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f30ea9",
   "metadata": {},
   "source": [
    "## 1. Load and Process Listings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3833804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 03:07:22,702 - INFO - Loaded listings from pickle: ../data/processed/etap2/listings_e2_df.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING LISTINGS DATA\n",
      "==================================================\n",
      "\n",
      "âœ“ Listings loaded successfully: (47572, 75)\n",
      "  Source: Pickle\n",
      "  Columns: ['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id']...\n",
      "  Warnings: Completely empty columns: ['neighbourhood_group_cleansed', 'calendar_updated', 'license']\n",
      "\n",
      "Data Preview:\n",
      "                    id                                      listing_url  \\\n",
      "0             18974065             https://www.nocarz.pl/rooms/18974065   \n",
      "1  1074535877676155362  https://www.nocarz.pl/rooms/1074535877676155362   \n",
      "2   796304692597746009   https://www.nocarz.pl/rooms/796304692597746009   \n",
      "\n",
      "        scrape_id last_scraped       source  \\\n",
      "0  20241211032909   2024-12-11  city scrape   \n",
      "1  20241211032909   2024-12-11  city scrape   \n",
      "2  20241211032909   2024-12-11  city scrape   \n",
      "\n",
      "                                    name  \\\n",
      "0  Charming split-level one bedroom flat   \n",
      "1             Stunning two bed apartment   \n",
      "2                  Southall, West London   \n",
      "\n",
      "                                         description  \\\n",
      "0  Large living room with open-plan kitchen. With...   \n",
      "1  This stylish place to stay is perfect for corp...   \n",
      "2  This stylish and elegant apartment consists of...   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0  Residential. Outside the train station there i...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "\n",
      "                                         picture_url    host_id  ...  \\\n",
      "0  https://a0.muscache.com/pictures/866de5f0-05a7...    5249313  ...   \n",
      "1  https://a0.muscache.com/pictures/hosting/Hosti...   90831525  ...   \n",
      "2  https://a0.muscache.com/pictures/miso/Hosting-...  491814778  ...   \n",
      "\n",
      "  review_scores_communication review_scores_location review_scores_value  \\\n",
      "0                         5.0                   4.63                4.74   \n",
      "1                         5.0                   4.50                4.00   \n",
      "2                         NaN                    NaN                 NaN   \n",
      "\n",
      "  license instant_bookable calculated_host_listings_count  \\\n",
      "0     NaN                f                              1   \n",
      "1     NaN                f                              3   \n",
      "2     NaN                t                             12   \n",
      "\n",
      "  calculated_host_listings_count_entire_homes  \\\n",
      "0                                           1   \n",
      "1                                           1   \n",
      "2                                          12   \n",
      "\n",
      "  calculated_host_listings_count_private_rooms  \\\n",
      "0                                            0   \n",
      "1                                            2   \n",
      "2                                            0   \n",
      "\n",
      "  calculated_host_listings_count_shared_rooms reviews_per_month  \n",
      "0                                           0              0.21  \n",
      "1                                           0              0.25  \n",
      "2                                           0               NaN  \n",
      "\n",
      "[3 rows x 75 columns]\n",
      "\n",
      "âœ“ Listings loaded successfully: (47572, 75)\n",
      "  Source: Pickle\n",
      "  Columns: ['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id']...\n",
      "  Warnings: Completely empty columns: ['neighbourhood_group_cleansed', 'calendar_updated', 'license']\n",
      "\n",
      "Data Preview:\n",
      "                    id                                      listing_url  \\\n",
      "0             18974065             https://www.nocarz.pl/rooms/18974065   \n",
      "1  1074535877676155362  https://www.nocarz.pl/rooms/1074535877676155362   \n",
      "2   796304692597746009   https://www.nocarz.pl/rooms/796304692597746009   \n",
      "\n",
      "        scrape_id last_scraped       source  \\\n",
      "0  20241211032909   2024-12-11  city scrape   \n",
      "1  20241211032909   2024-12-11  city scrape   \n",
      "2  20241211032909   2024-12-11  city scrape   \n",
      "\n",
      "                                    name  \\\n",
      "0  Charming split-level one bedroom flat   \n",
      "1             Stunning two bed apartment   \n",
      "2                  Southall, West London   \n",
      "\n",
      "                                         description  \\\n",
      "0  Large living room with open-plan kitchen. With...   \n",
      "1  This stylish place to stay is perfect for corp...   \n",
      "2  This stylish and elegant apartment consists of...   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0  Residential. Outside the train station there i...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "\n",
      "                                         picture_url    host_id  ...  \\\n",
      "0  https://a0.muscache.com/pictures/866de5f0-05a7...    5249313  ...   \n",
      "1  https://a0.muscache.com/pictures/hosting/Hosti...   90831525  ...   \n",
      "2  https://a0.muscache.com/pictures/miso/Hosting-...  491814778  ...   \n",
      "\n",
      "  review_scores_communication review_scores_location review_scores_value  \\\n",
      "0                         5.0                   4.63                4.74   \n",
      "1                         5.0                   4.50                4.00   \n",
      "2                         NaN                    NaN                 NaN   \n",
      "\n",
      "  license instant_bookable calculated_host_listings_count  \\\n",
      "0     NaN                f                              1   \n",
      "1     NaN                f                              3   \n",
      "2     NaN                t                             12   \n",
      "\n",
      "  calculated_host_listings_count_entire_homes  \\\n",
      "0                                           1   \n",
      "1                                           1   \n",
      "2                                          12   \n",
      "\n",
      "  calculated_host_listings_count_private_rooms  \\\n",
      "0                                            0   \n",
      "1                                            2   \n",
      "2                                            0   \n",
      "\n",
      "  calculated_host_listings_count_shared_rooms reviews_per_month  \n",
      "0                                           0              0.21  \n",
      "1                                           0              0.25  \n",
      "2                                           0               NaN  \n",
      "\n",
      "[3 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define your logger, EXPECTED_COLUMNS, MIN_EXPECTED_ROWS, listings_path_e2, and listings_pkl_path_e2 before this code\n",
    "\n",
    "def load_and_process_listings(csv_path, pickle_path, force_reload=False):\n",
    "    \"\"\"Load and process listings data with comprehensive validation.\"\"\"\n",
    "\n",
    "    # Check if we should use pickle\n",
    "    use_pickle = (os.path.exists(pickle_path) and\n",
    "                  check_file_freshness(csv_path, pickle_path) and\n",
    "                  not force_reload)\n",
    "\n",
    "    try:\n",
    "        if use_pickle:\n",
    "            df = pd.read_pickle(pickle_path)\n",
    "            logger.info(f\"Loaded listings from pickle: {pickle_path}\")\n",
    "            loaded_from_pickle = True\n",
    "        else:\n",
    "            logger.info(f\"Loading listings from CSV: {csv_path}\")\n",
    "            df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "            # Basic data cleaning\n",
    "            if 'price' in df.columns:\n",
    "                # Clean price column if it exists\n",
    "                df['price_cleaned'] = df['price'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "                df['price_cleaned'] = pd.to_numeric(df['price_cleaned'], errors='coerce')\n",
    "\n",
    "            loaded_from_pickle = False\n",
    "            logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
    "\n",
    "        # Validate data\n",
    "        validation = validate_dataframe(df, 'listings', EXPECTED_COLUMNS['listings'], MIN_EXPECTED_ROWS['listings'])\n",
    "\n",
    "        # Save to pickle if loaded from CSV and validation passed\n",
    "        if not loaded_from_pickle and validation['valid']:\n",
    "            df.to_pickle(pickle_path)\n",
    "            logger.info(f\"Saved listings to pickle: {pickle_path}\")\n",
    "\n",
    "        return df, validation, loaded_from_pickle\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading listings: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load listings data\n",
    "print(\"=\"*50)\n",
    "print(\"LOADING LISTINGS DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    listings_e2_df, listings_validation, loaded_from_pickle = load_and_process_listings(\n",
    "        listings_path_e2, listings_pkl_path_e2\n",
    "    )\n",
    "    print(f\"\\nâœ“ Listings loaded successfully: {listings_e2_df.shape}\")\n",
    "    print(f\"  Source: {'Pickle' if loaded_from_pickle else 'CSV'}\")\n",
    "    print(f\"  Columns: {list(listings_e2_df.columns)[:10]}...\" if len(listings_e2_df.columns) > 10 else f\"  Columns: {list(listings_e2_df.columns)}\")\n",
    "\n",
    "    # Show validation results\n",
    "    if listings_validation['warnings']:\n",
    "        print(\"  Warnings:\", \"; \".join(listings_validation['warnings']))\n",
    "    if listings_validation['errors']:\n",
    "        print(\"  Errors:\", \"; \".join(listings_validation['errors']))\n",
    "\n",
    "    # Quick data preview\n",
    "    print(\"\\nData Preview:\")\n",
    "    print(listings_e2_df.head(3))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to load listings: {e}\")\n",
    "    listings_e2_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "905d6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar_e2_df loaded successfully from pickle file: ../data/processed/etap2/calendar_e2_df.pkl\n",
      "\n",
      "Head of calendar_e2_df:\n",
      "            listing_id       date  available    price  price_cleaned\n",
      "0             19902160 2025-05-02      False   $37.00           37.0\n",
      "1             37075766 2025-11-25      False   $72.00           72.0\n",
      "2  1091268028307262502 2025-05-28       True  $311.00          311.0\n",
      "3   829621911605471263 2025-12-03       True   $36.00           36.0\n",
      "4              4223187 2025-09-16      False   $86.00           86.0\n",
      "\n",
      "Info for calendar_e2_df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17363102 entries, 0 to 17363101\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   listing_id     int64         \n",
      " 1   date           datetime64[ns]\n",
      " 2   available      bool          \n",
      " 3   price          object        \n",
      " 4   price_cleaned  float64       \n",
      "dtypes: bool(1), datetime64[ns](1), float64(1), int64(1), object(1)\n",
      "memory usage: 546.4+ MB\n",
      "\n",
      "Number of duplicate rows in calendar_e2_df: 0\n",
      "\n",
      "==================================================\n",
      "LOADING CALENDAR DATA\n",
      "==================================================\n",
      "\n",
      "Number of duplicate rows in calendar_e2_df: 0\n",
      "\n",
      "==================================================\n",
      "LOADING CALENDAR DATA\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 03:07:32,945 - INFO - Loaded calendar from pickle: ../data/processed/etap2/calendar_e2_df.pkl\n",
      "2025-06-26 03:07:38,052 - INFO - Calendar date range: 2024-12-11 00:00:00 to 2025-12-21 00:00:00\n",
      "2025-06-26 03:07:38,052 - INFO - Calendar date range: 2024-12-11 00:00:00 to 2025-12-21 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Calendar loaded successfully: (17363102, 5)\n",
      "  Source: Pickle\n",
      "  Columns: ['listing_id', 'date', 'available', 'price', 'price_cleaned']\n",
      "\n",
      "Data Preview:\n",
      "            listing_id       date  available    price  price_cleaned\n",
      "0             19902160 2025-05-02      False   $37.00           37.0\n",
      "1             37075766 2025-11-25      False   $72.00           72.0\n",
      "2  1091268028307262502 2025-05-28       True  $311.00          311.0\n"
     ]
    }
   ],
   "source": [
    "loaded_calendar_from_pickle = False\n",
    "try:\n",
    "    if os.path.exists(calendar_pkl_path_e2):\n",
    "        calendar_e2_df = pd.read_pickle(calendar_pkl_path_e2)\n",
    "        print(f\"calendar_e2_df loaded successfully from pickle file: {calendar_pkl_path_e2}\")\n",
    "        loaded_calendar_from_pickle = True\n",
    "    else:\n",
    "        print(f\"Pickle file not found: {calendar_pkl_path_e2}. Loading from CSV and processing...\")\n",
    "        cols_to_load_calendar = ['listing_id', 'date', 'available', 'price']\n",
    "        calendar_e2_df = pd.read_csv(calendar_path_e2, usecols=cols_to_load_calendar)\n",
    "        print(f\"{calendar_path_e2} loaded successfully.\")\n",
    "\n",
    "        # Process 'available'\n",
    "        if 'available' in calendar_e2_df.columns:\n",
    "            calendar_e2_df['available'] = calendar_e2_df['available'].map({'t': True, 'f': False}).astype(bool)\n",
    "\n",
    "        # Process 'price' into 'price_cleaned'\n",
    "        if 'price' in calendar_e2_df.columns and calendar_e2_df['price'].dtype == 'object':\n",
    "            calendar_e2_df['price_cleaned'] = calendar_e2_df['price'].replace({'[\\\\$,]': ''}, regex=True).astype(float)\n",
    "\n",
    "        # Process 'date'\n",
    "        if 'date' in calendar_e2_df.columns:\n",
    "            calendar_e2_df['date'] = pd.to_datetime(calendar_e2_df['date'])\n",
    "        print(\"Essential processing for calendar_e2_df completed.\")\n",
    "\n",
    "    print(\"\\nHead of calendar_e2_df:\")\n",
    "    print(calendar_e2_df.head())\n",
    "    print(\"\\nInfo for calendar_e2_df:\")\n",
    "    calendar_e2_df.info()\n",
    "    print(f\"\\nNumber of duplicate rows in calendar_e2_df: {calendar_e2_df.duplicated().sum()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing calendar_e2_df: {e}\")\n",
    "\n",
    "def load_and_process_calendar(csv_path, pickle_path, force_reload=False):\n",
    "    \"\"\"Load and process calendar data with optimizations.\"\"\"\n",
    "\n",
    "    use_pickle = (os.path.exists(pickle_path) and\n",
    "                  check_file_freshness(csv_path, pickle_path) and\n",
    "                  not force_reload)\n",
    "\n",
    "    try:\n",
    "        if use_pickle:\n",
    "            df = pd.read_pickle(pickle_path)\n",
    "            logger.info(f\"Loaded calendar from pickle: {pickle_path}\")\n",
    "            loaded_from_pickle = True\n",
    "        else:\n",
    "            logger.info(f\"Loading calendar from CSV: {csv_path}\")\n",
    "            # Load only essential columns for memory efficiency\n",
    "            cols_to_load = ['listing_id', 'date', 'available', 'price']\n",
    "            df = pd.read_csv(csv_path, usecols=cols_to_load)\n",
    "\n",
    "            # Process data\n",
    "            if 'available' in df.columns:\n",
    "                df['available'] = df['available'].map({'t': True, 'f': False}).astype(bool)\n",
    "\n",
    "            if 'price' in df.columns and df['price'].dtype == 'object':\n",
    "                df['price_cleaned'] = df['price'].str.replace(r'[\\$,]', '', regex=True)\n",
    "                df['price_cleaned'] = pd.to_numeric(df['price_cleaned'], errors='coerce')\n",
    "\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "            loaded_from_pickle = False\n",
    "            logger.info(f\"Processed {len(df)} calendar rows\")\n",
    "\n",
    "        # Validate\n",
    "        validation = validate_dataframe(df, 'calendar', EXPECTED_COLUMNS['calendar'], MIN_EXPECTED_ROWS['calendar'])\n",
    "\n",
    "        # Check for date range issues\n",
    "        if 'date' in df.columns:\n",
    "            date_range = df['date'].agg(['min', 'max'])\n",
    "            logger.info(f\"Calendar date range: {date_range['min']} to {date_range['max']}\")\n",
    "\n",
    "            # Check for missing dates per listing\n",
    "            listing_date_counts = df.groupby('listing_id')['date'].count()\n",
    "            short_periods = (listing_date_counts < 30).sum()\n",
    "            if short_periods > 0:\n",
    "                validation['warnings'].append(f\"{short_periods} listings with <30 days of data\")\n",
    "\n",
    "        if not loaded_from_pickle and validation['valid']:\n",
    "            df.to_pickle(pickle_path)\n",
    "            logger.info(f\"Saved calendar to pickle: {pickle_path}\")\n",
    "\n",
    "        return df, validation, loaded_from_pickle\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading calendar: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load calendar data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING CALENDAR DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    calendar_e2_df, calendar_validation, loaded_from_pickle = load_and_process_calendar(\n",
    "        calendar_path_e2, calendar_pkl_path_e2\n",
    "    )\n",
    "    print(f\"\\nâœ“ Calendar loaded successfully: {calendar_e2_df.shape}\")\n",
    "    print(f\"  Source: {'Pickle' if loaded_from_pickle else 'CSV'}\")\n",
    "    print(f\"  Columns: {list(calendar_e2_df.columns)}\")\n",
    "\n",
    "    # Show validation results\n",
    "    if calendar_validation['warnings']:\n",
    "        print(\"  Warnings:\", \"; \".join(calendar_validation['warnings']))\n",
    "    if calendar_validation['errors']:\n",
    "        print(\"  Errors:\", \"; \".join(calendar_validation['errors']))\n",
    "\n",
    "    # Quick data preview\n",
    "    print(\"\\nData Preview:\")\n",
    "    print(calendar_e2_df.head(3))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to load calendar: {e}\")\n",
    "    calendar_e2_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87d4b4",
   "metadata": {},
   "source": [
    "## 3. Load and Process Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "352b5bcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:128\u001b[0;36m\u001b[0m\n\u001b[0;31m    if not loaded_from_pickle and validation['valid']:        # Save to pickle if loaded from CSV and validation passed\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "## 4. Cross-Dataset Validation & Summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    listings_e2_df = None    print(f\"âœ— Failed to load listings: {e}\")except Exception as e:        print(missing_summary.head(10))        print(\"\\nTop Missing Values:\")    if not missing_summary.empty:    missing_summary = get_missing_value_summary(listings_e2_df, 'listings')    # Missing value summary        print(listings_e2_df.head(3))    print(\"\\nData Preview:\")    # Quick data preview        print(\"  Errors:\", \"; \".join(listings_validation['errors']))    if listings_validation['errors']:        print(\"  Warnings:\", \"; \".join(listings_validation['warnings']))    if listings_validation['warnings']:    # Show validation results    print(f\"  Columns: {list(listings_e2_df.columns)[:10]}...\" if len(listings_e2_df.columns) > 10 else f\"  Columns: {list(listings_e2_df.columns)}\")    print(f\"  Source: {'Pickle' if loaded_from_pickle else 'CSV'}\")    print(f\"\\nâœ“ Listings loaded successfully: {listings_e2_df.shape}\")    )        listings_path_e2, listings_pkl_path_e2    listings_e2_df, listings_validation, loaded_from_pickle = load_and_process_listings(try:print(\"=\"*50)print(\"LOADING LISTINGS DATA\")print(\"=\"*50)# Load listings data        raise        logger.error(f\"Error loading listings: {e}\")    except Exception as e:        return df, validation, loaded_from_pickle            logger.info(f\"Saved listings to pickle: {pickle_path}\")            df.to_pickle(pickle_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    reviews_e2_df = None    print(f\"âœ— Failed to load reviews: {e}\")except Exception as e:        print(missing_summary.head(5))        print(\"\\nTop Missing Values:\")    if not missing_summary.empty:    missing_summary = get_missing_value_summary(reviews_e2_df, 'reviews')    # Missing value summary        print(reviews_e2_df.head(3))\n",
    "    print(\"\\nData Preview:\")    # Quick data preview            print(\"  Errors:\", \"; \".join(reviews_validation['errors']))    if reviews_validation['errors']:        print(\"  Warnings:\", \"; \".join(reviews_validation['warnings']))\n",
    "    if reviews_validation['warnings']:    # Show validation results        print(f\"  Columns: {list(reviews_e2_df.columns)}\")    print(f\"  Source: {'Pickle' if loaded_from_pickle else 'CSV'}\")    print(f\"\\nâœ“ Reviews loaded successfully: {reviews_e2_df.shape}\")\n",
    "\n",
    "    )        reviews_path_e2, reviews_pkl_path_e2    reviews_e2_df, reviews_validation, loaded_from_pickle = load_and_process_reviews(\n",
    "try:print(\"=\"*50)print(\"LOADING REVIEWS DATA\")print(\"\\n\" + \"=\"*50)# Load reviews data        raise        logger.error(f\"Error loading reviews: {e}\")    except Exception as e:\n",
    "\n",
    "\n",
    "\n",
    "            return df, validation, loaded_from_pickle                    logger.info(f\"Saved reviews to pickle: {pickle_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            df.to_pickle(pickle_path)        if not loaded_from_pickle and validation['valid']:        # Save to pickle if loaded from CSV and validation passed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            logger.info(f\"Reviews date range: {date_range['min']} to {date_range['max']}\")\n",
    "            date_range = df['date'].agg(['min', 'max'])        if 'date' in df.columns:        # Check date range                validation = validate_dataframe(df, 'reviews', EXPECTED_COLUMNS['reviews'], MIN_EXPECTED_ROWS['reviews'])        # Validate                    logger.info(f\"Processed {len(df)} review rows\")\n",
    "        if not loaded_from_pickle and validation['valid']:        # Save to pickle if loaded from CSV and validation passed\n",
    "\n",
    "            loaded_from_pickle = False                            df['comment_length'] = df['comments'].str.len()\n",
    "        validation = validate_dataframe(df, 'listings', EXPECTED_COLUMNS['listings'], MIN_EXPECTED_ROWS['listings'])                # Calculate comment length                df['comments'] = df['comments'].replace('nan', np.nan)\n",
    "\n",
    "        # Validate data            logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
    "                # Replace 'nan' strings with actual NaN            loaded_from_pickle = False                df['price_cleaned'] = pd.to_numeric(df['price_cleaned'], errors='coerce')                df['price_cleaned'] = df['price'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "\n",
    "\n",
    "                df['comments'] = df['comments'].astype(str).str.strip()\n",
    "                # Remove excessive whitespace            if 'comments' in df.columns:            # Basic text cleaning for comments                            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "                # Clean price column if it exists            if 'price' in df.columns:            if 'date' in df.columns:            # Process date column\n",
    "\n",
    "            # Basic data cleaning            df = pd.read_csv(csv_path, low_memory=False)            logger.info(f\"Loading listings from CSV: {csv_path}\")\n",
    "\n",
    "                        df = pd.read_csv(csv_path)        else:            loaded_from_pickle = True            logger.info(f\"Loaded listings from pickle: {pickle_path}\")            df = pd.read_pickle(pickle_path)\n",
    "\n",
    "            logger.info(f\"Loading reviews from CSV: {csv_path}\")\n",
    "        else:\n",
    "            loaded_from_pickle = True            logger.info(f\"Loaded reviews from pickle: {pickle_path}\")        if use_pickle:    try:                  not force_reload)                  check_file_freshness(csv_path, pickle_path) and    use_pickle = (os.path.exists(pickle_path) and    # Check if we should use pickle    \"\"\"Load and process listings data with comprehensive validation.\"\"\"def load_and_process_listings(csv_path, pickle_path, force_reload=False):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            df = pd.read_pickle(pickle_path)        if use_pickle:\n",
    "    try:                      not force_reload)def load_and_process_reviews(csv_path, pickle_path, force_reload=False):\n",
    "\n",
    "                  check_file_freshness(csv_path, pickle_path) and\n",
    "    use_pickle = (os.path.exists(pickle_path) and    \"\"\"Load and process reviews data with validation and text cleaning.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de3873",
   "metadata": {},
   "source": [
    "## 4. Cross-Dataset Validation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_dataset_validation():\n",
    "    \"\"\"Perform validation across all three datasets.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CROSS-DATASET VALIDATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    results = {\n",
    "        'datasets_loaded': [],\n",
    "        'id_overlaps': {},\n",
    "        'join_success_rates': {},\n",
    "        'total_unique_ids': {},\n",
    "        'orphan_records': {}\n",
    "    }\n",
    "\n",
    "    # Check which datasets are available\n",
    "    datasets = {\n",
    "        'listings': listings_e2_df if 'listings_e2_df' in globals() and listings_e2_df is not None else None,\n",
    "        'calendar': calendar_e2_df if 'calendar_e2_df' in globals() and calendar_e2_df is not None else None,\n",
    "        'reviews': reviews_e2_df if 'reviews_e2_df' in globals() and reviews_e2_df is not None else None\n",
    "    }\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        if df is not None:\n",
    "            results['datasets_loaded'].append(name)\n",
    "            print(f\"âœ“ {name}: {df.shape}\")\n",
    "        else:\n",
    "            print(f\"âœ— {name}: Not loaded\")\n",
    "\n",
    "    # ID Analysis\n",
    "    if datasets['listings'] is not None:\n",
    "        listings_ids = set(datasets['listings']['id'].dropna())\n",
    "        results['total_unique_ids']['listings'] = len(listings_ids)\n",
    "        print(f\"\\nðŸ“Š Unique listing IDs in listings.csv: {len(listings_ids):,}\")\n",
    "\n",
    "    if datasets['calendar'] is not None:\n",
    "        calendar_ids = set(datasets['calendar']['listing_id'].dropna())\n",
    "        results['total_unique_ids']['calendar'] = len(calendar_ids)\n",
    "        print(f\"ðŸ“Š Unique listing IDs in calendar.csv: {len(calendar_ids):,}\")\n",
    "\n",
    "    if datasets['reviews'] is not None:\n",
    "        reviews_ids = set(datasets['reviews']['listing_id'].dropna())\n",
    "        results['total_unique_ids']['reviews'] = len(reviews_ids)\n",
    "        print(f\"ðŸ“Š Unique listing IDs in reviews.csv: {len(reviews_ids):,}\")\n",
    "\n",
    "    # Join Analysis\n",
    "    if datasets['listings'] is not None and datasets['calendar'] is not None:\n",
    "        overlap = listings_ids.intersection(calendar_ids)\n",
    "        results['id_overlaps']['listings_calendar'] = len(overlap)\n",
    "        results['join_success_rates']['listings_calendar'] = len(overlap) / len(listings_ids) * 100\n",
    "\n",
    "        orphan_calendar = len(calendar_ids - listings_ids)\n",
    "        results['orphan_records']['calendar'] = orphan_calendar\n",
    "\n",
    "        print(f\"\\nðŸ”— Listings-Calendar Join:\")\n",
    "        print(f\"   Successful joins: {len(overlap):,} ({len(overlap)/len(listings_ids)*100:.1f}% of listings)\")\n",
    "        print(f\"   Orphan calendar records: {orphan_calendar:,}\")\n",
    "\n",
    "    if datasets['listings'] is not None and datasets['reviews'] is not None:\n",
    "        overlap = listings_ids.intersection(reviews_ids)\n",
    "        results['id_overlaps']['listings_reviews'] = len(overlap)\n",
    "        results['join_success_rates']['listings_reviews'] = len(overlap) / len(listings_ids) * 100\n",
    "\n",
    "        orphan_reviews = len(reviews_ids - listings_ids)\n",
    "        results['orphan_records']['reviews'] = orphan_reviews\n",
    "\n",
    "        print(f\"\\nðŸ”— Listings-Reviews Join:\")\n",
    "        print(f\"   Successful joins: {len(overlap):,} ({len(overlap)/len(listings_ids)*100:.1f}% of listings)\")\n",
    "        print(f\"   Orphan review records: {orphan_reviews:,}\")\n",
    "\n",
    "    # Three-way join\n",
    "    if all(df is not None for df in datasets.values()):\n",
    "        three_way = listings_ids.intersection(calendar_ids).intersection(reviews_ids)\n",
    "        results['id_overlaps']['all_three'] = len(three_way)\n",
    "\n",
    "        print(f\"\\nðŸŽ¯ Three-way Join (All datasets):\")\n",
    "        print(f\"   Complete records: {len(three_way):,} listings\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Perform validation\n",
    "validation_results = perform_cross_dataset_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"Generate comprehensive summary report.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Data loading summary\n",
    "    print(\"\\nðŸ“‹ Data Loading Summary:\")\n",
    "    for dataset in validation_results['datasets_loaded']:\n",
    "        print(f\"   âœ“ {dataset.title()} data loaded successfully\")\n",
    "\n",
    "    # Key metrics comparison (vs Etap 1 findings)\n",
    "    print(\"\\nðŸ“Š Key Metrics (vs Etap 1 baseline):\")\n",
    "\n",
    "    if 'listings' in validation_results['total_unique_ids']:\n",
    "        listings_count = validation_results['total_unique_ids']['listings']\n",
    "        print(f\"   â€¢ Total listings: {listings_count:,}\")\n",
    "\n",
    "    if 'all_three' in validation_results['id_overlaps']:\n",
    "        complete_records = validation_results['id_overlaps']['all_three']\n",
    "        print(f\"   â€¢ Complete records (all 3 datasets): {complete_records:,} (Etap 1: ~11,000)\")\n",
    "\n",
    "    total_orphans = sum(validation_results['orphan_records'].values())\n",
    "    if total_orphans > 0:\n",
    "        print(f\"   â€¢ Total orphan records: {total_orphans:,} (Etap 1: ~75,000)\")\n",
    "\n",
    "    # Critical missing values check\n",
    "    if 'listings_e2_df' in globals() and listings_e2_df is not None:\n",
    "        missing_neighbourhood = listings_e2_df['neighbourhood_cleansed'].isna().sum()\n",
    "        missing_neighbourhood_pct = (missing_neighbourhood / len(listings_e2_df)) * 100\n",
    "        print(f\"   â€¢ Missing neighbourhood_cleansed: {missing_neighbourhood:,} ({missing_neighbourhood_pct:.1f}%) (Etap 1: ~30%)\")\n",
    "\n",
    "        # Check other critical features\n",
    "        critical_features = ['bedrooms', 'bathrooms_text', 'beds', 'latitude', 'longitude']\n",
    "        for feature in critical_features:\n",
    "            if feature in listings_e2_df.columns:\n",
    "                missing_count = listings_e2_df[feature].isna().sum()\n",
    "                missing_pct = (missing_count / len(listings_e2_df)) * 100\n",
    "                print(f\"   â€¢ Missing {feature}: {missing_count:,} ({missing_pct:.1f}%)\")\n",
    "\n",
    "    # Calendar observation periods\n",
    "    if 'calendar_e2_df' in globals() and calendar_e2_df is not None:\n",
    "        listing_periods = calendar_e2_df.groupby('listing_id')['date'].count()\n",
    "        median_period = listing_periods.median()\n",
    "        short_periods = (listing_periods < 30).sum()\n",
    "        short_periods_pct = (short_periods / len(listing_periods)) * 100\n",
    "\n",
    "        print(f\"   â€¢ Median observation period: {median_period:.0f} days (Etap 1: below a year)\")\n",
    "        print(f\"   â€¢ Listings with <30 days: {short_periods:,} ({short_periods_pct:.1f}%) (Etap 1: significant portion)\")\n",
    "\n",
    "    # Data quality assessment\n",
    "    print(\"\\nðŸŽ¯ Data Quality Assessment:\")\n",
    "\n",
    "    # Calculate improvement scores\n",
    "    improvements = []\n",
    "    concerns = []\n",
    "\n",
    "    if complete_records > 11000:\n",
    "        improvements.append(\"âœ“ More complete records available for analysis\")\n",
    "\n",
    "    if total_orphans < 75000:\n",
    "        improvements.append(\"âœ“ Reduced orphan records\")\n",
    "\n",
    "    if 'listings_e2_df' in globals() and missing_neighbourhood_pct < 30:\n",
    "        improvements.append(\"âœ“ Improved neighbourhood_cleansed completeness\")\n",
    "\n",
    "    if improvements:\n",
    "        for improvement in improvements:\n",
    "            print(f\"   {improvement}\")\n",
    "\n",
    "    if concerns:\n",
    "        for concern in concerns:\n",
    "            print(f\"   {concern}\")\n",
    "\n",
    "    print(\"\\nðŸ“ Next Steps:\")\n",
    "    print(\"   1. Proceed to etap2_02_data_validation_profiling.ipynb\")\n",
    "    print(\"   2. Detailed missing value analysis and schema verification\")\n",
    "    print(\"   3. Comprehensive comparison with Etap 1 findings\")\n",
    "    print(\"   4. Assessment of imputation strategy requirements\")\n",
    "\n",
    "    # Save summary to file\n",
    "    summary_path = os.path.join(processed_data_dir_e2, 'data_loading_summary.txt')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(f\"Etap 2 Data Loading Summary - {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(f\"Datasets loaded: {', '.join(validation_results['datasets_loaded'])}\\n\")\n",
    "        f.write(f\"Validation results saved to: {summary_path}\\n\")\n",
    "\n",
    "    print(f\"\\nðŸ’¾ Summary saved to: {summary_path}\")\n",
    "\n",
    "# Generate final summary\n",
    "generate_summary_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
