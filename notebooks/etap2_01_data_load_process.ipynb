{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5f1efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data directory '../data/processed/etap2/' ensured.\n",
      "✓ Setup completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "listings_path_e2 = \"../data/raw2/listings.csv\"\n",
    "calendar_path_e2 = \"../data/raw2/calendar.csv\"\n",
    "reviews_path_e2 = \"../data/raw2/reviews.csv\"\n",
    "\n",
    "\n",
    "processed_data_dir_e2 = \"../data/processed/etap2/\"\n",
    "listings_pkl_path_e2 = os.path.join(processed_data_dir_e2, \"listings_e2_df.pkl\")\n",
    "calendar_pkl_path_e2 = os.path.join(processed_data_dir_e2, \"calendar_e2_df.pkl\")\n",
    "reviews_pkl_path_e2 = os.path.join(processed_data_dir_e2, \"reviews_e2_df.pkl\")\n",
    "\n",
    "\n",
    "os.makedirs(processed_data_dir_e2, exist_ok=True)\n",
    "print(f\"Processed data directory '{processed_data_dir_e2}' ensured.\")\n",
    "\n",
    "\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"listings\": [\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"host_id\",\n",
    "        \"neighbourhood_cleansed\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"property_type\",\n",
    "        \"room_type\",\n",
    "        \"price\",\n",
    "    ],\n",
    "    \"calendar\": [\"listing_id\", \"date\", \"available\", \"price\"],\n",
    "    \"reviews\": [\"listing_id\", \"id\", \"date\", \"reviewer_id\", \"reviewer_name\", \"comments\"],\n",
    "}\n",
    "\n",
    "MIN_EXPECTED_ROWS = {\"listings\": 10000, \"calendar\": 100000, \"reviews\": 50000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "140c2bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def check_file_freshness(csv_path, pickle_path):\n",
    "    \"\"\"Check if pickle file is newer than CSV file.\"\"\"\n",
    "    if not os.path.exists(csv_path) or not os.path.exists(pickle_path):\n",
    "        return False\n",
    "    return os.path.getmtime(pickle_path) > os.path.getmtime(csv_path)\n",
    "\n",
    "\n",
    "def validate_dataframe(df, name, expected_cols, min_rows):\n",
    "    \"\"\"Validate dataframe structure and content.\"\"\"\n",
    "    validation = {\"valid\": True, \"warnings\": [], \"errors\": []}\n",
    "\n",
    "    if len(df) < min_rows:\n",
    "        validation[\"warnings\"].append(\n",
    "            f\"Only {len(df)} rows, expected at least {min_rows}\"\n",
    "        )\n",
    "\n",
    "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        validation[\"warnings\"].append(f\"Missing expected columns: {missing_cols}\")\n",
    "\n",
    "    empty_cols = [col for col in df.columns if df[col].isna().all()]\n",
    "    if empty_cols:\n",
    "        validation[\"warnings\"].append(f\"Completely empty columns: {empty_cols}\")\n",
    "\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        validation[\"warnings\"].append(f\"{duplicates} duplicate rows found\")\n",
    "\n",
    "    return validation\n",
    "\n",
    "\n",
    "def get_missing_value_summary(df, name):\n",
    "    \"\"\"Get comprehensive missing value summary.\"\"\"\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_pct = (missing_counts / len(df)) * 100\n",
    "\n",
    "    summary = pd.DataFrame({\"count\": missing_counts, \"percentage\": missing_pct})\n",
    "\n",
    "    return summary[summary[\"count\"] > 0].sort_values(\"percentage\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f30ea9",
   "metadata": {},
   "source": [
    "## 1. Load and Process Listings Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3833804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 03:07:22,702 - INFO - Loaded listings from pickle: ../data/processed/etap2/listings_e2_df.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING LISTINGS DATA\n",
      "==================================================\n",
      "\n",
      "✓ Listings loaded successfully: (47572, 75)\n",
      "  Source: Pickle\n",
      "  Columns: ['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id']...\n",
      "  Warnings: Completely empty columns: ['neighbourhood_group_cleansed', 'calendar_updated', 'license']\n",
      "\n",
      "Data Preview:\n",
      "                    id                                      listing_url  \\\n",
      "0             18974065             https://www.nocarz.pl/rooms/18974065   \n",
      "1  1074535877676155362  https://www.nocarz.pl/rooms/1074535877676155362   \n",
      "2   796304692597746009   https://www.nocarz.pl/rooms/796304692597746009   \n",
      "\n",
      "        scrape_id last_scraped       source  \\\n",
      "0  20241211032909   2024-12-11  city scrape   \n",
      "1  20241211032909   2024-12-11  city scrape   \n",
      "2  20241211032909   2024-12-11  city scrape   \n",
      "\n",
      "                                    name  \\\n",
      "0  Charming split-level one bedroom flat   \n",
      "1             Stunning two bed apartment   \n",
      "2                  Southall, West London   \n",
      "\n",
      "                                         description  \\\n",
      "0  Large living room with open-plan kitchen. With...   \n",
      "1  This stylish place to stay is perfect for corp...   \n",
      "2  This stylish and elegant apartment consists of...   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0  Residential. Outside the train station there i...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "\n",
      "                                         picture_url    host_id  ...  \\\n",
      "0  https://a0.muscache.com/pictures/866de5f0-05a7...    5249313  ...   \n",
      "1  https://a0.muscache.com/pictures/hosting/Hosti...   90831525  ...   \n",
      "2  https://a0.muscache.com/pictures/miso/Hosting-...  491814778  ...   \n",
      "\n",
      "  review_scores_communication review_scores_location review_scores_value  \\\n",
      "0                         5.0                   4.63                4.74   \n",
      "1                         5.0                   4.50                4.00   \n",
      "2                         NaN                    NaN                 NaN   \n",
      "\n",
      "  license instant_bookable calculated_host_listings_count  \\\n",
      "0     NaN                f                              1   \n",
      "1     NaN                f                              3   \n",
      "2     NaN                t                             12   \n",
      "\n",
      "  calculated_host_listings_count_entire_homes  \\\n",
      "0                                           1   \n",
      "1                                           1   \n",
      "2                                          12   \n",
      "\n",
      "  calculated_host_listings_count_private_rooms  \\\n",
      "0                                            0   \n",
      "1                                            2   \n",
      "2                                            0   \n",
      "\n",
      "  calculated_host_listings_count_shared_rooms reviews_per_month  \n",
      "0                                           0              0.21  \n",
      "1                                           0              0.25  \n",
      "2                                           0               NaN  \n",
      "\n",
      "[3 rows x 75 columns]\n",
      "\n",
      "✓ Listings loaded successfully: (47572, 75)\n",
      "  Source: Pickle\n",
      "  Columns: ['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id']...\n",
      "  Warnings: Completely empty columns: ['neighbourhood_group_cleansed', 'calendar_updated', 'license']\n",
      "\n",
      "Data Preview:\n",
      "                    id                                      listing_url  \\\n",
      "0             18974065             https://www.nocarz.pl/rooms/18974065   \n",
      "1  1074535877676155362  https://www.nocarz.pl/rooms/1074535877676155362   \n",
      "2   796304692597746009   https://www.nocarz.pl/rooms/796304692597746009   \n",
      "\n",
      "        scrape_id last_scraped       source  \\\n",
      "0  20241211032909   2024-12-11  city scrape   \n",
      "1  20241211032909   2024-12-11  city scrape   \n",
      "2  20241211032909   2024-12-11  city scrape   \n",
      "\n",
      "                                    name  \\\n",
      "0  Charming split-level one bedroom flat   \n",
      "1             Stunning two bed apartment   \n",
      "2                  Southall, West London   \n",
      "\n",
      "                                         description  \\\n",
      "0  Large living room with open-plan kitchen. With...   \n",
      "1  This stylish place to stay is perfect for corp...   \n",
      "2  This stylish and elegant apartment consists of...   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0  Residential. Outside the train station there i...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "\n",
      "                                         picture_url    host_id  ...  \\\n",
      "0  https://a0.muscache.com/pictures/866de5f0-05a7...    5249313  ...   \n",
      "1  https://a0.muscache.com/pictures/hosting/Hosti...   90831525  ...   \n",
      "2  https://a0.muscache.com/pictures/miso/Hosting-...  491814778  ...   \n",
      "\n",
      "  review_scores_communication review_scores_location review_scores_value  \\\n",
      "0                         5.0                   4.63                4.74   \n",
      "1                         5.0                   4.50                4.00   \n",
      "2                         NaN                    NaN                 NaN   \n",
      "\n",
      "  license instant_bookable calculated_host_listings_count  \\\n",
      "0     NaN                f                              1   \n",
      "1     NaN                f                              3   \n",
      "2     NaN                t                             12   \n",
      "\n",
      "  calculated_host_listings_count_entire_homes  \\\n",
      "0                                           1   \n",
      "1                                           1   \n",
      "2                                          12   \n",
      "\n",
      "  calculated_host_listings_count_private_rooms  \\\n",
      "0                                            0   \n",
      "1                                            2   \n",
      "2                                            0   \n",
      "\n",
      "  calculated_host_listings_count_shared_rooms reviews_per_month  \n",
      "0                                           0              0.21  \n",
      "1                                           0              0.25  \n",
      "2                                           0               NaN  \n",
      "\n",
      "[3 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_and_process_listings(csv_path, pickle_path, force_reload=False):\n",
    "    \"\"\"Load and process listings data with comprehensive validation.\"\"\"\n",
    "\n",
    "    use_pickle = (\n",
    "        os.path.exists(pickle_path)\n",
    "        and check_file_freshness(csv_path, pickle_path)\n",
    "        and not force_reload\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if use_pickle:\n",
    "            df = pd.read_pickle(pickle_path)\n",
    "            logger.info(f\"Loaded listings from pickle: {pickle_path}\")\n",
    "            loaded_from_pickle = True\n",
    "        else:\n",
    "            logger.info(f\"Loading listings from CSV: {csv_path}\")\n",
    "            df = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "            if \"price\" in df.columns:\n",
    "\n",
    "                df[\"price_cleaned\"] = (\n",
    "                    df[\"price\"].astype(str).str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "                )\n",
    "                df[\"price_cleaned\"] = pd.to_numeric(\n",
    "                    df[\"price_cleaned\"], errors=\"coerce\"\n",
    "                )\n",
    "\n",
    "            loaded_from_pickle = False\n",
    "            logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
    "\n",
    "        validation = validate_dataframe(\n",
    "            df, \"listings\", EXPECTED_COLUMNS[\"listings\"], MIN_EXPECTED_ROWS[\"listings\"]\n",
    "        )\n",
    "\n",
    "        if not loaded_from_pickle and validation[\"valid\"]:\n",
    "            df.to_pickle(pickle_path)\n",
    "            logger.info(f\"Saved listings to pickle: {pickle_path}\")\n",
    "\n",
    "        return df, validation, loaded_from_pickle\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading listings: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOADING LISTINGS DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    listings_e2_df, listings_validation, loaded_from_pickle = load_and_process_listings(\n",
    "        listings_path_e2, listings_pkl_path_e2\n",
    "    )\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    listings_e2_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "905d6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar_e2_df loaded successfully from pickle file: ../data/processed/etap2/calendar_e2_df.pkl\n",
      "\n",
      "Head of calendar_e2_df:\n",
      "            listing_id       date  available    price  price_cleaned\n",
      "0             19902160 2025-05-02      False   $37.00           37.0\n",
      "1             37075766 2025-11-25      False   $72.00           72.0\n",
      "2  1091268028307262502 2025-05-28       True  $311.00          311.0\n",
      "3   829621911605471263 2025-12-03       True   $36.00           36.0\n",
      "4              4223187 2025-09-16      False   $86.00           86.0\n",
      "\n",
      "Info for calendar_e2_df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17363102 entries, 0 to 17363101\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   listing_id     int64         \n",
      " 1   date           datetime64[ns]\n",
      " 2   available      bool          \n",
      " 3   price          object        \n",
      " 4   price_cleaned  float64       \n",
      "dtypes: bool(1), datetime64[ns](1), float64(1), int64(1), object(1)\n",
      "memory usage: 546.4+ MB\n",
      "\n",
      "Number of duplicate rows in calendar_e2_df: 0\n",
      "\n",
      "==================================================\n",
      "LOADING CALENDAR DATA\n",
      "==================================================\n",
      "\n",
      "Number of duplicate rows in calendar_e2_df: 0\n",
      "\n",
      "==================================================\n",
      "LOADING CALENDAR DATA\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 03:07:32,945 - INFO - Loaded calendar from pickle: ../data/processed/etap2/calendar_e2_df.pkl\n",
      "2025-06-26 03:07:38,052 - INFO - Calendar date range: 2024-12-11 00:00:00 to 2025-12-21 00:00:00\n",
      "2025-06-26 03:07:38,052 - INFO - Calendar date range: 2024-12-11 00:00:00 to 2025-12-21 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Calendar loaded successfully: (17363102, 5)\n",
      "  Source: Pickle\n",
      "  Columns: ['listing_id', 'date', 'available', 'price', 'price_cleaned']\n",
      "\n",
      "Data Preview:\n",
      "            listing_id       date  available    price  price_cleaned\n",
      "0             19902160 2025-05-02      False   $37.00           37.0\n",
      "1             37075766 2025-11-25      False   $72.00           72.0\n",
      "2  1091268028307262502 2025-05-28       True  $311.00          311.0\n"
     ]
    }
   ],
   "source": [
    "loaded_calendar_from_pickle = False\n",
    "try:\n",
    "\n",
    "    if os.path.exists(calendar_pkl_path_e2):\n",
    "        calendar_e2_df = pd.read_pickle(calendar_pkl_path_e2)\n",
    "        print(\n",
    "            f\"calendar_e2_df loaded successfully from pickle file: {calendar_pkl_path_e2}\"\n",
    "        )\n",
    "        loaded_calendar_from_pickle = True\n",
    "    else:\n",
    "        print(\n",
    "            f\"Pickle file not found: {calendar_pkl_path_e2}. Loading from CSV and processing...\"\n",
    "        )\n",
    "        cols_to_load_calendar = [\"listing_id\", \"date\", \"available\", \"price\"]\n",
    "        calendar_e2_df = pd.read_csv(calendar_path_e2, usecols=cols_to_load_calendar)\n",
    "        print(f\"{calendar_path_e2} loaded successfully.\")\n",
    "\n",
    "        if \"available\" in calendar_e2_df.columns:\n",
    "            calendar_e2_df[\"available\"] = (\n",
    "                calendar_e2_df[\"available\"].map({\"t\": True, \"f\": False}).astype(bool)\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            \"price\" in calendar_e2_df.columns\n",
    "            and calendar_e2_df[\"price\"].dtype == \"object\"\n",
    "        ):\n",
    "            calendar_e2_df[\"price_cleaned\"] = (\n",
    "                calendar_e2_df[\"price\"]\n",
    "                .replace({\"[\\\\$,]\": \"\"}, regex=True)\n",
    "                .astype(float)\n",
    "            )\n",
    "\n",
    "        if \"date\" in calendar_e2_df.columns:\n",
    "            calendar_e2_df[\"date\"] = pd.to_datetime(calendar_e2_df[\"date\"])\n",
    "        # print(\"Essential processing for calendar_e2_df completed.\")\n",
    "\n",
    "    print(\"\\nHead of calendar_e2_df:\")\n",
    "    print(calendar_e2_df.head())\n",
    "    print(\"\\nInfo for calendar_e2_df:\")\n",
    "    calendar_e2_df.info()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing calendar_e2_df: {e}\")\n",
    "\n",
    "\n",
    "def load_and_process_calendar(csv_path, pickle_path, force_reload=False):\n",
    "    \"\"\"Load and process calendar data with optimizations.\"\"\"\n",
    "\n",
    "    use_pickle = (\n",
    "        os.path.exists(pickle_path)\n",
    "        and check_file_freshness(csv_path, pickle_path)\n",
    "        and not force_reload\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        if use_pickle:\n",
    "            df = pd.read_pickle(pickle_path)\n",
    "            logger.info(f\"Loaded calendar from pickle: {pickle_path}\")\n",
    "            loaded_from_pickle = True\n",
    "        else:\n",
    "            logger.info(f\"Loading calendar from CSV: {csv_path}\")\n",
    "\n",
    "            cols_to_load = [\"listing_id\", \"date\", \"available\", \"price\"]\n",
    "            df = pd.read_csv(csv_path, usecols=cols_to_load)\n",
    "\n",
    "            if \"available\" in df.columns:\n",
    "                df[\"available\"] = (\n",
    "                    df[\"available\"].map({\"t\": True, \"f\": False}).astype(bool)\n",
    "                )\n",
    "\n",
    "            if \"price\" in df.columns and df[\"price\"].dtype == \"object\":\n",
    "                df[\"price_cleaned\"] = df[\"price\"].str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "                df[\"price_cleaned\"] = pd.to_numeric(\n",
    "                    df[\"price_cleaned\"], errors=\"coerce\"\n",
    "                )\n",
    "\n",
    "            if \"date\" in df.columns:\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "            loaded_from_pickle = False\n",
    "            logger.info(f\"Processed {len(df)} calendar rows\")\n",
    "\n",
    "        validation = validate_dataframe(\n",
    "            df, \"calendar\", EXPECTED_COLUMNS[\"calendar\"], MIN_EXPECTED_ROWS[\"calendar\"]\n",
    "        )\n",
    "\n",
    "        if \"date\" in df.columns:\n",
    "            date_range = df[\"date\"].agg([\"min\", \"max\"])\n",
    "            logger.info(\n",
    "                f\"Calendar date range: {date_range['min']} to {date_range['max']}\"\n",
    "            )\n",
    "\n",
    "            listing_date_counts = df.groupby(\"listing_id\")[\"date\"].count()\n",
    "            short_periods = (listing_date_counts < 30).sum()\n",
    "            if short_periods > 0:\n",
    "                validation[\"warnings\"].append(\n",
    "                    f\"{short_periods} listings with <30 days of data\"\n",
    "                )\n",
    "\n",
    "        if not loaded_from_pickle and validation[\"valid\"]:\n",
    "            df.to_pickle(pickle_path)\n",
    "            logger.info(f\"Saved calendar to pickle: {pickle_path}\")\n",
    "\n",
    "        return df, validation, loaded_from_pickle\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading calendar: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"LOADING CALENDAR DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    calendar_e2_df, calendar_validation, loaded_from_pickle = load_and_process_calendar(\n",
    "        calendar_path_e2, calendar_pkl_path_e2\n",
    "    )\n",
    "    print(f\"\\n✓ Calendar loaded successfully: {calendar_e2_df.shape}\")\n",
    "    print(f\"  Source: {'Pickle' if loaded_from_pickle else 'CSV'}\")\n",
    "    print(f\"  Columns: {list(calendar_e2_df.columns)}\")\n",
    "\n",
    "    if calendar_validation[\"warnings\"]:\n",
    "        print(\"  Warnings:\", \"; \".join(calendar_validation[\"warnings\"]))\n",
    "    if calendar_validation[\"errors\"]:\n",
    "        print(\"  Errors:\", \"; \".join(calendar_validation[\"errors\"]))\n",
    "\n",
    "    print(\"\\nData Preview:\")\n",
    "    print(calendar_e2_df.head(3))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load calendar: {e}\")\n",
    "    calendar_e2_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87d4b4",
   "metadata": {},
   "source": [
    "## 3. Load and Process Reviews Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "352b5bcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:128\u001b[0;36m\u001b[0m\n\u001b[0;31m    if not loaded_from_pickle and validation['valid']:        # Save to pickle if loaded from CSV and validation passed\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_reviews(csv_path, pickle_path, force_reload=False):\n",
    "    \"\"\"Load and process reviews data with validation and text cleaning.\"\"\"\n",
    "\n",
    "    use_pickle = (\n",
    "        os.path.exists(pickle_path)\n",
    "        and check_file_freshness(csv_path, pickle_path)\n",
    "        and not force_reload\n",
    "    )\n",
    "\n",
    "    if use_pickle:\n",
    "        logger.info(f\"Loading reviews from fresh pickle: {pickle_path}\")\n",
    "        df = pd.read_pickle(pickle_path)\n",
    "        loaded_from_pickle = True\n",
    "    else:\n",
    "        logger.info(f\"Loading reviews from CSV: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        loaded_from_pickle = False\n",
    "\n",
    "        if \"comments\" in df.columns:\n",
    "\n",
    "            df[\"comments\"] = df[\"comments\"].astype(str).str.strip()\n",
    "\n",
    "            df[\"comment_length\"] = df[\"comments\"].str.len()\n",
    "\n",
    "    validation = validate_dataframe(\n",
    "        df, \"reviews\", EXPECTED_COLUMNS[\"reviews\"], MIN_EXPECTED_ROWS[\"reviews\"]\n",
    "    )\n",
    "\n",
    "    if not loaded_from_pickle and validation[\"valid\"]:\n",
    "        df.to_pickle(pickle_path)\n",
    "        logger.info(f\"Saved processed reviews to pickle: {pickle_path}\")\n",
    "\n",
    "    return df, validation, loaded_from_pickle\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"LOADING REVIEWS DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    reviews_e2_df, reviews_validation, loaded_from_pickle = load_and_process_reviews(\n",
    "        reviews_path_e2, reviews_pkl_path_e2\n",
    "    )\n",
    "\n",
    "    if reviews_validation[\"valid\"]:\n",
    "        source = \"Pickle\" if loaded_from_pickle else \"CSV\"\n",
    "        print(f\"Reviews loaded successfully: {reviews_e2_df.shape}\")\n",
    "        print(f\"Source: {source}\")\n",
    "\n",
    "        if reviews_validation[\"warnings\"]:\n",
    "            for warning in reviews_validation[\"warnings\"]:\n",
    "                print(f\"  Warning: {warning}\")\n",
    "\n",
    "        missing_summary = get_missing_value_summary(reviews_e2_df, \"reviews\")\n",
    "        if not missing_summary.empty:\n",
    "            print(\"\\nTop Missing Values:\")\n",
    "            print(missing_summary.head(5))\n",
    "\n",
    "        if \"date\" in reviews_e2_df.columns:\n",
    "\n",
    "            reviews_e2_df[\"date\"] = pd.to_datetime(\n",
    "                reviews_e2_df[\"date\"], errors=\"coerce\"\n",
    "            )\n",
    "            date_range = reviews_e2_df[\"date\"].agg([\"min\", \"max\"])\n",
    "            logger.info(\n",
    "                f\"Reviews date range: {date_range['min'].date()} to {date_range['max'].date()}\"\n",
    "            )\n",
    "\n",
    "        print(\"\\nData Preview:\")\n",
    "        print(reviews_e2_df.head(3))\n",
    "\n",
    "    else:\n",
    "        print(\"Validation failed:\")\n",
    "        for error in reviews_validation.get(\"errors\", []):\n",
    "            print(f\" - {error}\")\n",
    "        for warning in reviews_validation.get(\"warnings\", []):\n",
    "            print(f\" - {warning}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ An unexpected error occurred while loading reviews: {e}\")\n",
    "    reviews_e2_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de3873",
   "metadata": {},
   "source": [
    "## 4. Cross-Dataset Validation & Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_dataset_validation():\n",
    "    \"\"\"Perform validation across all three datasets.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CROSS-DATASET VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    results = {\n",
    "        \"datasets_loaded\": [],\n",
    "        \"id_overlaps\": {},\n",
    "        \"join_success_rates\": {},\n",
    "        \"total_unique_ids\": {},\n",
    "        \"orphan_records\": {},\n",
    "    }\n",
    "\n",
    "    datasets = {\n",
    "        \"listings\": (\n",
    "            listings_e2_df\n",
    "            if \"listings_e2_df\" in globals() and listings_e2_df is not None\n",
    "            else None\n",
    "        ),\n",
    "        \"calendar\": (\n",
    "            calendar_e2_df\n",
    "            if \"calendar_e2_df\" in globals() and calendar_e2_df is not None\n",
    "            else None\n",
    "        ),\n",
    "        \"reviews\": (\n",
    "            reviews_e2_df\n",
    "            if \"reviews_e2_df\" in globals() and reviews_e2_df is not None\n",
    "            else None\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        if df is not None:\n",
    "            results[\"datasets_loaded\"].append(name)\n",
    "            print(f\"✓ {name}: {df.shape}\")\n",
    "        else:\n",
    "            print(f\"✗ {name}: Not loaded\")\n",
    "\n",
    "    if datasets[\"listings\"] is not None:\n",
    "        listings_ids = set(datasets[\"listings\"][\"id\"].dropna())\n",
    "        results[\"total_unique_ids\"][\"listings\"] = len(listings_ids)\n",
    "        print(f\"\\nUnique listing IDs in listings.csv: {len(listings_ids):,}\")\n",
    "\n",
    "    if datasets[\"calendar\"] is not None:\n",
    "        calendar_ids = set(datasets[\"calendar\"][\"listing_id\"].dropna())\n",
    "        results[\"total_unique_ids\"][\"calendar\"] = len(calendar_ids)\n",
    "        print(f\"Unique listing IDs in calendar.csv: {len(calendar_ids):,}\")\n",
    "\n",
    "    if datasets[\"reviews\"] is not None:\n",
    "        reviews_ids = set(datasets[\"reviews\"][\"listing_id\"].dropna())\n",
    "        results[\"total_unique_ids\"][\"reviews\"] = len(reviews_ids)\n",
    "        print(f\"Unique listing IDs in reviews.csv: {len(reviews_ids):,}\")\n",
    "\n",
    "    if datasets[\"listings\"] is not None and datasets[\"calendar\"] is not None:\n",
    "        overlap = listings_ids.intersection(calendar_ids)\n",
    "        results[\"id_overlaps\"][\"listings_calendar\"] = len(overlap)\n",
    "        results[\"join_success_rates\"][\"listings_calendar\"] = (\n",
    "            len(overlap) / len(listings_ids) * 100\n",
    "        )\n",
    "\n",
    "        orphan_calendar = len(calendar_ids - listings_ids)\n",
    "        results[\"orphan_records\"][\"calendar\"] = orphan_calendar\n",
    "\n",
    "        print(\n",
    "            f\"   Successful joins: {len(overlap):,} ({len(overlap)/len(listings_ids)*100:.1f}% of listings)\"\n",
    "        )\n",
    "        print(f\"   Orphan calendar records: {orphan_calendar:,}\")\n",
    "\n",
    "    if datasets[\"listings\"] is not None and datasets[\"reviews\"] is not None:\n",
    "        overlap = listings_ids.intersection(reviews_ids)\n",
    "        results[\"id_overlaps\"][\"listings_reviews\"] = len(overlap)\n",
    "        results[\"join_success_rates\"][\"listings_reviews\"] = (\n",
    "            len(overlap) / len(listings_ids) * 100\n",
    "        )\n",
    "\n",
    "        orphan_reviews = len(reviews_ids - listings_ids)\n",
    "        results[\"orphan_records\"][\"reviews\"] = orphan_reviews\n",
    "\n",
    "        print(f\"\\n🔗 Listings-Reviews Join:\")\n",
    "        print(\n",
    "            f\"   Successful joins: {len(overlap):,} ({len(overlap)/len(listings_ids)*100:.1f}% of listings)\"\n",
    "        )\n",
    "        print(f\"   Orphan review records: {orphan_reviews:,}\")\n",
    "\n",
    "    if all(df is not None for df in datasets.values()):\n",
    "        three_way = listings_ids.intersection(calendar_ids).intersection(reviews_ids)\n",
    "        results[\"id_overlaps\"][\"all_three\"] = len(three_way)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "validation_results = perform_cross_dataset_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"Generate comprehensive summary report.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Data loading summary\n",
    "    for dataset in validation_results[\"datasets_loaded\"]:\n",
    "        print(f\"{dataset.title()} data loaded successfully\")\n",
    "\n",
    "    if \"listings\" in validation_results[\"total_unique_ids\"]:\n",
    "        listings_count = validation_results[\"total_unique_ids\"][\"listings\"]\n",
    "\n",
    "    if \"all_three\" in validation_results[\"id_overlaps\"]:\n",
    "        complete_records = validation_results[\"id_overlaps\"][\"all_three\"]\n",
    "        print(\n",
    "            f\"   • Complete records (all 3 datasets): {complete_records:,} (Etap 1: ~11,000)\"\n",
    "        )\n",
    "\n",
    "    total_orphans = sum(validation_results[\"orphan_records\"].values())\n",
    "    if total_orphans > 0:\n",
    "        print(f\"   • Total orphan records: {total_orphans:,} (Etap 1: ~75,000)\")\n",
    "\n",
    "    if \"listings_e2_df\" in globals() and listings_e2_df is not None:\n",
    "        missing_neighbourhood = listings_e2_df[\"neighbourhood_cleansed\"].isna().sum()\n",
    "        missing_neighbourhood_pct = (missing_neighbourhood / len(listings_e2_df)) * 100\n",
    "        print(\n",
    "            f\"   • Missing neighbourhood_cleansed: {missing_neighbourhood:,} ({missing_neighbourhood_pct:.1f}%) (Etap 1: ~30%)\"\n",
    "        )\n",
    "\n",
    "        critical_features = [\n",
    "            \"bedrooms\",\n",
    "            \"bathrooms_text\",\n",
    "            \"beds\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "        ]\n",
    "        for feature in critical_features:\n",
    "            if feature in listings_e2_df.columns:\n",
    "                missing_count = listings_e2_df[feature].isna().sum()\n",
    "                missing_pct = (missing_count / len(listings_e2_df)) * 100\n",
    "                print(f\"   • Missing {feature}: {missing_count:,} ({missing_pct:.1f}%)\")\n",
    "\n",
    "    if \"calendar_e2_df\" in globals() and calendar_e2_df is not None:\n",
    "        listing_periods = calendar_e2_df.groupby(\"listing_id\")[\"date\"].count()\n",
    "        median_period = listing_periods.median()\n",
    "        short_periods = (listing_periods < 30).sum()\n",
    "        short_periods_pct = (short_periods / len(listing_periods)) * 100\n",
    "\n",
    "        print(\n",
    "            f\"   • Median observation period: {median_period:.0f} days (Etap 1: below a year)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   • Listings with <30 days: {short_periods:,} ({short_periods_pct:.1f}%) (Etap 1: significant portion)\"\n",
    "        )\n",
    "\n",
    "    improvements = []\n",
    "    concerns = []\n",
    "\n",
    "    if complete_records > 11000:\n",
    "        improvements.append(\"✓ More complete records available for analysis\")\n",
    "\n",
    "    if total_orphans < 75000:\n",
    "        improvements.append(\"✓ Reduced orphan records\")\n",
    "\n",
    "    if \"listings_e2_df\" in globals() and missing_neighbourhood_pct < 30:\n",
    "        improvements.append(\"✓ Improved neighbourhood_cleansed completeness\")\n",
    "\n",
    "    if improvements:\n",
    "        for improvement in improvements:\n",
    "            print(f\"   {improvement}\")\n",
    "\n",
    "    if concerns:\n",
    "        for concern in concerns:\n",
    "            print(f\"   {concern}\")\n",
    "\n",
    "    summary_path = os.path.join(processed_data_dir_e2, \"data_loading_summary.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(\n",
    "            f\"Etap 2 Data Loading Summary - {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\"\n",
    "        )\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(\n",
    "            f\"Datasets loaded: {', '.join(validation_results['datasets_loaded'])}\\n\"\n",
    "        )\n",
    "        f.write(f\"Validation results saved to: {summary_path}\\n\")\n",
    "\n",
    "\n",
    "generate_summary_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
